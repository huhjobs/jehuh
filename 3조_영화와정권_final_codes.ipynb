{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3조_영화와정권_final_codes.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"cvVL1GcdLpdd"},"source":["!pip install selenium==3.11.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fxMBBafguhJ5"},"source":["import pandas as pd\r\n","\r\n","from selenium import webdriver\r\n","from selenium.webdriver.common.by import By\r\n","from selenium.webdriver.common.keys import Keys\r\n","\r\n","import requests\r\n","from bs4 import BeautifulSoup\r\n","\r\n","import time\r\n","\r\n","from konlpy.tag import Twitter\r\n","from collections import Counter\r\n","\r\n","from PIL import Image \r\n","import numpy as np\r\n","from wordcloud import WordCloud\r\n","import matplotlib.pyplot as plt\r\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ll1j0Tn2FWKJ"},"source":["### PART I. 데이터 전처리 (movie_df.csv)\r\n",": get_code()\r\n"]},{"cell_type":"code","metadata":{"id":"vOGBREaUHOZY"},"source":["# 영화관입장권통합전산망(KOBIS)에서 역대 한국영화 박스오피스 순위 자료 수집\r\n","# (@http://www.kobis.or.kr/kobis/business/main/main.do)\r\n","# dataframe으로 저장\r\n","\r\n","movie_list = pd.DataFrame([])\r\n","movied_released_years = [str(year).zfill(2) for year in range(4,21)]\r\n","\r\n","for movie_released in movied_released_years:\r\n","    korean_movie = pd.read_html('KOBIS_20{}.xls'.format(movie_released),encoding='utf-8')\r\n","    korean_movie = korean_movie[1].head(10)\r\n","    movie_list = movie_list.append(korean_movie)\r\n","\r\n","\r\n","movie_list.to_csv('korean_movie_ranking_04_20.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JcV2EVTJbvB8"},"source":["korean_movie_list_df = pd.read_csv('korean_movie_ranking_04_20.csv')\r\n","korean_movie_list_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v7gVd8Kx-_id"},"source":["# '개봉일'의 yyyy-mm-dd 형식을 yyyy 형식으로 변환하여 '개봉년'에 저장\n","# dataframe에서 불필요한 column 제거 (순위','매출액','매출액 점유율','관객수', '스크린수', '상영횟수','대표국적','국적','배급사')\n","# '영화코드' column 생성\n","\n","korean_movie_list_df = pd.read_csv('korean_movie_ranking_04_20.csv', index_col=0)\n","korean_movie_list_df['개봉년'] = korean_movie_list_df['개봉일'].str.split('-', 3, expand=True)[0]\n","korean_movie_list_df = korean_movie_list_df.drop(['순위','개봉일','매출액','매출액 점유율'], axis=1)\n","movie_df_with_03 = korean_movie_list_df.drop(['관객수', '스크린수', '상영횟수','대표국적','국적','배급사'], axis=1)\n","movie_df = movie_df_with_03.replace(2003, 2004)\n","movie_df['영화코드'] = ''\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PgUza8ZCHRbh"},"source":["# get_code() 함수 : 영화명 리스트를 input으로 영화코드 리스트를 return\r\n","# 네이버 영화 사이트의 html형식이 영화별로 상이하여 trouble shooting 기법 사용\r\n","# get_code() --> get_code_error() --> get_code_error_error()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u2WYLfkY_nRP"},"source":["def get_code1(movie_name_list):\r\n","    \r\n","    movie_code_list = []\r\n","    driver = webdriver.Chrome(executable_path='(driver) chromedriver.exe')\r\n","    naver_movie_url = 'https://movie.naver.com/' # 네이버 영화 링크\r\n","    driver.get(naver_movie_url)\r\n","    \r\n","    for movie_name in movie_name_list:\r\n","        \r\n","        year = str(list(movie_df['개봉년'][movie_df['영화명'] == movie_name])[0])\r\n","        driver.find_element_by_id('ipt_tx_srch').send_keys(movie_name + year) # (영화이름 + 년도)로 검색\r\n","        driver.find_element_by_class_name('btn_srch').click()\r\n","        \r\n","        web = driver.page_source\r\n","        source = BeautifulSoup(web, 'html.parser')\r\n","        time.sleep(2)\r\n","        \r\n","        if source.find('p', {'class' : 'result_thumb'}) == None: # 검색 결과 페이지 최상단에 영화가 없을 때\r\n","            movie_code_list.append('000000') # 코드 대신 에러 메시지('000000') 입력\r\n","        else: # 검색 결과 페이지 최상단에 영화가 있을 때\r\n","            movie_code = source.find('p', {'class' : 'result_thumb'}).find('a').attrs['href'][-6:] \r\n","            #consider code length : 5 or 6\r\n","            if movie_code.startswith('='):\r\n","                movie_code_list.append(movie_code[1:])\r\n","            else:\r\n","                movie_code_list.append(movie_code)\r\n","            \r\n","    driver.close()\r\n","    driver.quit()\r\n","    \r\n","    return movie_code_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MmnNv0k__ygU"},"source":["def get_code2(movie_name_list): \r\n","    \r\n","    movie_code_list = []\r\n","    driver = webdriver.Chrome(executable_path='(driver) chromedriver.exe')\r\n","    naver_movie_url = 'https://movie.naver.com/' # 네이버 영화 링크\r\n","    driver.get(naver_movie_url)\r\n","    \r\n","    for movie_name in movie_name_list:\r\n","        \r\n","        year = str(list(movie_df['개봉년'][movie_df['영화명'] == movie_name])[0])\r\n","        driver.find_element_by_id('ipt_tx_srch').send_keys(movie_name)      \r\n","        driver.find_element_by_class_name('btn_srch').click()\r\n","        \r\n","        web = driver.page_source\r\n","        source = BeautifulSoup(web, 'html.parser')\r\n","        time.sleep(2)\r\n","\r\n","        # 검색 결과 최상단의 영화의 발매년도가 원하는 영화의 발매년도과 일치한다면 코드 추출\r\n","        # 네이버 영화에 발매년도가 1년 작게 등록된 경우들이 있어서 year과 year-1 모두 고려\r\n","\r\n","        if (source.find('ul',{'class':'search_list_1'}).find('dd',{'class':\"etc\"}).find_all('a')[-1].get_text() != year) & (source.find('ul',{'class':'search_list_1'}).find('dd',{'class':\"etc\"}).find_all('a')[-1].get_text() != str(int(year)-1)) :\r\n","            movie_code_list.append('000001')  # 발매년도가 일치하지 않는다면 코드 대신 에러 메시지('000000') 입력\r\n","        else:\r\n","            movie_code = source.find('p', {'class' : 'result_thumb'}).find('a').attrs['href'][-6:]      \r\n","            #consider code length : 5 or 6\r\n","            if movie_code.startswith('='):\r\n","                movie_code_list.append(movie_code[1:])\r\n","            else:\r\n","                movie_code_list.append(movie_code)\r\n","            \r\n","    driver.close()\r\n","    driver.quit()\r\n","    \r\n","    return movie_code_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oAx4-qrB_1Ne"},"source":["def get_code3(movie_name_list):#영화 이름으로 검색 후 url\r\n","    movie_code_list = []\r\n","    driver = webdriver.Chrome(executable_path='(driver) chromedriver.exe')\r\n","    naver_movie_url = 'https://movie.naver.com/' # 네이버 영화 링크\r\n","    driver.get(naver_movie_url)\r\n","    \r\n","    for movie_name in movie_name_list:\r\n","        driver.find_element_by_id('ipt_tx_srch').send_keys(movie_name)\r\n","        driver.find_element_by_class_name('btn_srch').click()\r\n","        web = driver.page_source\r\n","        source = BeautifulSoup(web, 'html.parser')\r\n","        time.sleep(3)\r\n","        year = list(korean_movie_list_df['개봉일'][korean_movie_list_df['영화명'] == movie_name])[0][:4]\r\n","        \r\n","        # 검색 결과 중 개봉년도가 일치하는 영화의 코드 추출\r\n","        # 네이버 영화에 발매년도가 1년 작게 등록된 경우들이 있어서 year과 year-1 모두 고려\r\n","        # 세 방법이 모두 통하지 않는 영화는 과감히 버림.(Try-Except)\r\n","\r\n","        try:\r\n","            movies_html_list = source.find('ul',{'class':'search_list_1'}).find_all('dl')\r\n","            for i in range(len(movies_html_list)):\r\n","                movie_html = movies_html_list[i]\r\n","                if (movie_html.find('dd',{'class':\"etc\"}).find_all('a')[-1].get_text() == year) or (movie_html.find('dd',{'class':\"etc\"}).find_all('a')[-1].get_text() == str(int(year)-1)):\r\n","                    movie_code = movie_html.find('a').attrs['href'][-6:]\r\n","                    if movie_code.startswith('='):\r\n","                        movie_code_list.append(movie_code[1:])\r\n","                    else:\r\n","                        movie_code_list.append(movie_code)\r\n","                    break\r\n","        except:\r\n","            movie_code_list.append('')\r\n","            print('error in '+movie_name)\r\n","            \r\n","    driver.close()\r\n","    driver.quit()\r\n","    \r\n","    return movie_code_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DnZvjoQkX7QJ"},"source":["# get_code1() 실행하여 movie_df에 입력\r\n","movie_df['영화코드'] = get_code1(list(movie_df['영화명']))\r\n","temp1 = movie_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IvR5p31HAmyy"},"source":["# get_code1() 실행 후 error message('000000') 이 입력된 영화 목록에 대하여 get_code2() 실행 후 movie_df에 입력\r\n","error_list = get_code2(list(movie_df['영화명'][movie_df['영화코드'] =='000000']))#121\r\n","movie_df['영화코드'][movie_df['영화코드']=='000000'] = error_list\r\n","temp2 = movie_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7oY7vhKAA5Yc"},"source":["# get_code2() 실행 후 error message('000001') 이 입력된 영화 목록에 대하여 get_code3() 실행 후 movie_df에 입력\r\n","error_list = get_code3(list(movie_df['영화명'][movie_df['영화코드'] =='000001']))#121\r\n","movie_df['영화코드'][movie_df['영화코드']=='000001'] = error_list\r\n","temp3 = movie_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5j2sIwoLBZ5L"},"source":["# 영화명, 발매년, 영화코드 를 column으로 하는 완성된 dataframe을 'movie_df.csv'로 저장\r\n","movie_df.to_csv('movie_df.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xr-VhcuhOSm9"},"source":["### PART II. Web Scraping (new_movie_df.csv)\r\n",": review_url(), get_review(), get_actors()"]},{"cell_type":"code","metadata":{"id":"ZR0dBpmKZDeI"},"source":["new_movie_df = pd.read_csv('movie_df.csv')\r\n","new_movie_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RQGDhd0kxsPz"},"source":["# '영화리뷰url','영화리뷰','배우목록' column 생성\r\n","\r\n","new_movie_df['영화리뷰url'] = ''\r\n","new_movie_df['영화리뷰'] = ''\r\n","new_movie_df['배우목록'] = ''\r\n","new_movie_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eRKG2ELiZNHv"},"source":["# 해당 영화 리뷰 최대 100개의 url 불러와서 '영화리뷰url' column에 list 형식으로 추가\r\n","\r\n","def review_url(movie_name):\r\n","    idx = new_movie_df.index[new_movie_df['영화명'] == movie_name]\r\n","    movie_code = list(new_movie_df['영화코드'])[idx[0]]\r\n","    review_url_list = []\r\n","\r\n","    for page in range(5): # 영화 별로 리뷰페이지 5페이지까지 web scraping\r\n","        movie_review_page_url = 'https://movie.naver.com/movie/board/review/list.nhn?st=code&sword={}&od=goodcnt&page={}'.format(movie_code,str(page+1))\r\n","        web = requests.get(movie_review_page_url)\r\n","        source = BeautifulSoup(web.text, 'html.parser')  \r\n","            \r\n","        for i in range(len(source.find_all('td',{'class':'title'}))): # 리뷰 페이지에 보여지는 리뷰 게시글들의 url 불러오기\r\n","            url = 'https://movie.naver.com/movie/board/review/' + source.find_all('td',{'class':'title'})[i].find('a').get('href')\r\n","            review_url_list.append(url)\r\n","    \r\n","    new_movie_df['영화리뷰url'][idx[0]] = review_url_list # new_movie_df에 url 추가"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JFgefe05bmEs"},"source":["for movie_name in new_movie_df['영화명']:\r\n","    review_url(movie_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wgGEI9GtySLl"},"source":["# savepoint\r\n","\r\n","temp1 = new_movie_df\r\n","temp1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3OBi0avabCZR"},"source":["# 해당 영화 리뷰들 web scraping 후 '영화리뷰' column에 str 형식으로 추가\r\n","\r\n","def get_review(movie_name): # 영화    \r\n","    idx = new_movie_df.index[new_movie_df['영화명'] == movie_name]\r\n","    url_list = list(new_movie_df['영화리뷰url'])[idx[0]]\r\n","    \r\n","    review =''\r\n","    for url in url_list: # 각 리뷰(url)에서 리뷰본문.get_text()\r\n","        web = requests.get(url).content\r\n","        source = BeautifulSoup(web, 'html.parser')\r\n","        article = source.find('div', {'class': 'text_area'}).get_text() \r\n","        review += article\r\n","    \r\n","    #replace, encode.decode ; 본문 내용에서 쓸모없는 내용 제거\r\n","    review = review.replace(\"\\n\", \"\")\r\n","    review = review.replace(\"\\xa0\",\"\")\r\n","    review = review.replace(\"\\r\",\"\")\r\n","    review = review.replace(\"\\t\",\"\")\r\n","    review = review.replace(\" .text_area #post-view {width:500px;} /*블로그에서 템플릿쓰기를 했을경우 이미지를 연속으로 붙여 넣었을때 우리쪽에서 줄바꿈이 안되는 문제를 해결하기위함*/\",\"\")\r\n","    review = review.encode('utf-8', 'ignore').decode('utf-8') # removing emoji\r\n","    #blog 주소 삭제해야함\r\n","    \r\n","    new_movie_df['영화리뷰'][idx[0]] = review"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wv2kipieyrQO"},"source":["for movie_name in new_movie_df['영화명']:\r\n","    get_review(movie_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r9enxJ7UyspM"},"source":["# savepoint\r\n","\r\n","temp2 = new_movie_df\r\n","temp2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-vH6Ge-UHfqj"},"source":["# 해당 영화 감독 및 배우 이름 등 web scraping 후 '배우목록' column에 list 형식으로 추가\r\n","\r\n","def get_actors(movie_name):\r\n","    idx = new_movie_df.index[new_movie_df['영화명'] == movie_name]\r\n","    movie_code = list(new_movie_df['영화코드'])[idx[0]]\r\n","    url = 'https://movie.naver.com/movie/board/review/list.nhn?st=code&sword={}&od=goodcnt'.format(str(movie_code))\r\n","    web = requests.get(url)\r\n","    source = BeautifulSoup(web.text, 'html.parser')\r\n","    \r\n","    actor_list = []\r\n","    \r\n","    for i in range(len(source.find('table',{'class':'info_area'}).find_all('a'))):\r\n","        actor_list.append(source.find('table',{'class':'info_area'}).find_all('a')[i].get_text())\r\n","    \r\n","    new_movie_df['배우목록'][idx[0]] = actor_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RSlfWAdYHgcD"},"source":["for movie_name in new_movie_df['영화명']:\r\n","    get_actors(movie_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VPBiTxW74jHh"},"source":["#savepoint\r\n","\r\n","temp3 = new_movie_df\r\n","temp3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"51c7rKiyFC8Z"},"source":["# '영화명','개봉년','영화코드','영화리뷰url','영화리뷰','배우목록' 을 column으로 하는 dataframe 'new_movie_df.csv'로 저장\r\n","\r\n","new_movie_df.to_csv('new_movie_df.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5EZDqvUnYPoR"},"source":["### PART III. Wordcloud\r\n",": get_wordcounts(), get_word_dic(), wordcloud()"]},{"cell_type":"code","metadata":{"id":"cTkK2UpA_6l6"},"source":["df = pd.read_csv('new_movie_df.csv')\r\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bpQ1Lh93I3YW"},"source":["# 'word_dic' column 생성\r\n","\r\n","df['word_dic'] = ''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FUep8Ce_I5to"},"source":["# 보편적인 한국어 stopwords 불러오기\r\n","\r\n","stopwords_df = pd.read_json('stopwords-ko.json')\r\n","stopwords=list(stopwords_df[0])\r\n","stopwords"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yKw11maj3N6Z"},"source":["# 영화 별로 리뷰에서 나온 단어들을 빈도수 순서대로 정렬하고 dictionary 형식으로 저장하여 'word_dic' column에 추가\r\n","\r\n","def get_wordcounts(movie_name):\r\n","    twitter = Twitter()\r\n","    idx = df.index[df['영화명'] == movie_name]\r\n","    review_text = list(df['영화리뷰'])[idx[0]]\r\n","    raw_pos_tagged = twitter.pos(review_text, norm=True, stem=True) # POS Tagging\r\n","\r\n","    # 영화 제목에 나오는 단어, 주제와 관련없는 의미없는 단어 등을 del_list에 저장\r\n","    movie_name_keywords = list(dict(twitter.pos(movie_name, norm=True, stem=True)).keys())\r\n","    common_del_list = ['되어다','때문','그리고','좋다','자다','없다','같다','싶다','보다', '하다', '있다', '되다', '이다', '돼다', '않다', '그렇다', '아니다', '이렇다', '그렇다', '어떻다'] \r\n","    movie_del_list = ['영화', '배우','연기','개봉','출연','스틸컷','나오다','리뷰']\r\n","    del_list = stopwords + movie_name_keywords + eval(df['배우목록'][idx[0]]) + common_del_list + movie_del_list\r\n","\r\n","    # 의미없는 단어들을 모두 제외한 word_cleaned list 만들기\r\n","    word_cleaned = []\r\n","    for word in raw_pos_tagged: #  ('서울', 'Noun'),\r\n","        if not word[1] in [\"Josa\", \"Eomi\", \"Punctuation\", \"Foreign\"]: # Foreign == ”, “ 와 같이 제외되어야할 항목들\r\n","            if (len(word[0]) != 1) & (word[0] not in del_list) & (word[0] not in 'ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ'): # 한 글자로 이뤄진 단어들을 제외 & 원치 않는 단어들을 제외\r\n","                word_cleaned.append(word[0])\r\n","\r\n","    # 만들어진 word_cleaned list 속 단어들의 빈도수 계산 후 dict 형식으로 저장\r\n","    result = Counter(word_cleaned)\r\n","    word_dic = dict(result)\r\n","\r\n","    # 빈도 수에 따라 단어들 내림차순으로 정리\r\n","    sorted_word_dic = sorted(word_dic.items(), key=lambda x:x[1], reverse=True)\r\n","    \r\n","    # 'word_dic' column에 dict 형식으로 입력\r\n","    df['word_dic'][idx[0]] = dict(sorted_word_dic)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HwWvTc2gI9gP"},"source":["for movie_name in df['영화명']:\r\n","    get_wordcounts(movie_name)\r\n","  \r\n","df['word_dic']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZjbBD9yFwMxv"},"source":["# 시간 관계 상 4인이 연도별로 따로 get_wordcounts() 함수를 돌린 후 합침\r\n","df_0408 = pd.read_csv('df_0408.csv')\r\n","df_0913 = pd.read_csv('df_0913.csv')\r\n","df_1417 = pd.read_csv('df_1417.csv')\r\n","df_1820 = pd.read_csv('df_1820.csv')\r\n","\r\n","word_dic_0408 = list(df_0408[(df_0408['개봉년']>2002) & (df_0408['개봉년']<2009)]['word_dic'])\r\n","word_dic_0913 = list(df_0408[(df_0913['개봉년']>2008) & (df_0913['개봉년']<2014)]['word_dic'])\r\n","word_dic_1417 = list(df_1417[(df_1417['개봉년']>2013) & (df_1417['개봉년']<2018)]['word_dic'])\r\n","word_dic_1820 = list(df_1820[(df_1820['개봉년']>2017) & (df_1820['개봉년']<2021)]['word_dic'])\r\n","\r\n","word_dic = word_dic_0408 + word_dic_0913 + word_dic_1417 + word_dic_1820\r\n","\r\n","df['word_dic'] = word_dic"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xOPC15cBwQdF"},"source":["# '영화명','개봉년','영화코드','영화리뷰url','영화리뷰','배우목록', 'word_dic' 을 column으로 하는 dataframe 'count_df.csv'로 저장\r\n","df.to_csv('count_df.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ob7wXarGwvGl"},"source":["df = pd.read_csv('count_df.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lLdhypCPw6U-"},"source":["# 미쳐 지우지 못한 의미없는 단어들 delete_list에 추가하여 추후 삭제.\r\n","delete_list = {'늘다','보기','많이','많다','만들다','우리','크다','인물','작품','자신','하지만','느낌','가장','대해','보고','모습','사실','알다','그렇게','주인공','이야기','스토리','버리다','보여주다','정도','보이다','많이','극장','흥행','가다','들다','관객','모르다','느끼다','오다','위해','주다','받다','내다','그것','이렇게'}\r\n","\r\n","# year을 입력하면 당 해의 영화들의 word_dic을 병합하여(중복되는 단어의 빈도수를 합함) dict형식으로 return\r\n","# 영화명을 입력하면 해당되는 word_dic을 return  \r\n","def get_word_dic(para):\r\n","    if type(para) == int :\r\n","        word_dic = {}\r\n","        for dic in df['word_dic'][df['개봉년']==para]:\r\n","            try:\r\n","                for word in list(eval(dic).keys())[:50]:\r\n","                    if word not in delete_list:\r\n","                        if word not in list(word_dic.keys()):\r\n","                            word_dic[word] = eval(dic)[word]  \r\n","                        else:\r\n","                            word_dic[word] += eval(dic)[word]\r\n","            except:\r\n","                continue\r\n","        print('{}년의 sorted word dictionary가 생성되었습니다'.format(str(para)))\r\n","    \r\n","    elif type(para) == str :\r\n","        word_dic = df['word_dic'][df['영화명']==para]\r\n","        print('영화 {}의 sorted word dictionary가 생성되었습니다'.format(para))\r\n","    \r\n","    return word_dic"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YPQ0vxAj95av"},"source":["# 년도 혹은 영화명과 원하는 색깔(colormap형식으로)을 입력하면, 해당되는 리뷰들의 wordcloud 생성, image 파일로 저장\r\n","\r\n","def wordcloud(para, color) :\r\n","    peninsula_coloring = np.array(Image.open(\"peninsula.jpg\"))\r\n","    \r\n","    word_cloud = WordCloud(font_path=\"BMEuljiro10yearslater.ttf\", # font_path=\"C:/Windows/Fonts/NanumSquareB.ttf\"\r\n","                           width=2000, height=1000, # 이 부분을 수정하시면 실제 워드클라우드의 크기가 바뀝니다 (해상도가 바뀝니다)\r\n","                           # prefer_horizontal= 1.0, # 이 부분의 주석을 해제하시면 단어들이 가로로만 그려지게 됩니다. (0~1)\r\n","                           max_words=70, # 단어 70개만 표시\r\n","                           mask=peninsula_coloring,\r\n","                           background_color='black').generate_from_frequencies(get_word_dic(para)) # 단어 빈도수 기준으로 워드클라우드 제작\r\n","\r\n","    plt.figure(figsize=(15,15)) # 이 부분을 수정하시면 화면에서 보여지는 워드클라우드의 크기가 바뀝니다 \r\n","    plt.imshow(word_cloud.recolor(colormap=color)) # 워드 색깔 ('Reds', 'Blues')\r\n","    plt.axis(\"off\")\r\n","    plt.tight_layout(pad=0)\r\n","    plt.show()\r\n","    plt.savefig('wordcloud_{}.png'.format(para))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wCTSLnPTaFl0"},"source":["left = [2004,2005,2006,2007,2017,2018,2019,2020] # 진보(노무현, 문재인 정권)\r\n","right = [2008,2009,2010,2011,2012,2013,2014,2015,2016] # 보수(이명박, 박근혜 정권)\r\n","\r\n","for year in left:\r\n","    wordcloud(year,'Blues')\r\n","for year in right:\r\n","    wordcloud(year,'Reds')"],"execution_count":null,"outputs":[]}]}